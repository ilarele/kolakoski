{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_enc(d_model, max_position):\n",
    "    \"\"\"\n",
    "    :param d_model: dimension of the model\n",
    "    :param length: length of positions\n",
    "    :return: length*d_model position matrix\n",
    "    \"\"\"\n",
    "    if d_model % 2 != 0:\n",
    "        raise ValueError(\"Cannot use sin/cos positional encoding with \"\n",
    "                         \"odd dim (got dim={:d})\".format(d_model))\n",
    "    pe = torch.zeros(max_position, d_model)\n",
    "    position = torch.arange(0, max_position).unsqueeze(1)\n",
    "    div_term = torch.exp((torch.arange(0, d_model, 2, dtype=torch.float) *\n",
    "                         -(math.log(10000.0) / d_model)))\n",
    "    pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "\n",
    "    return pe\n",
    "\n",
    "def normalize(dataset):\n",
    "    mean, std = dataset.mean(), dataset.std()\n",
    "    result = (dataset - mean)/std\n",
    "\n",
    "    print(\"min, max\", result.min(), result.max())\n",
    "    print(\"mean, std\", mean, std)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enc_size = 20\n",
    "dir_path = \"dataset/1e8_unique\"\n",
    "dataset_np = np.load(\"%s/kolakosky.npy\" % dir_path).astype(np.float32)\n",
    "idxs_train = np.load(\"%s/train_idxs.npy\" % dir_path)\n",
    "idxs_valid = np.load(\"%s/valid_idxs.npy\" % dir_path)\n",
    "\n",
    "# normalize numbers\n",
    "dataset_np[:, 1:-1] = normalize(dataset_np[:, 1:-1])\n",
    "\n",
    "positional_encodings = get_pos_enc(pos_enc_size, int(dataset_np[:, 0].max()) + 1)\n",
    "\n",
    "print(dataset_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_np[0, 1:-1], dataset_np[:, 1:-1].min(), dataset_np[:, 1:-1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "trainset = torch.Tensor(np.hstack([positional_encodings[dataset_np[idxs_train, 0]], dataset_np[idxs_train, 1:]]))\n",
    "validset = torch.Tensor(np.hstack([positional_encodings[dataset_np[idxs_valid, 0]], dataset_np[idxs_valid, 1:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "batch_size = 1500\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=len(validset), shuffle=False, num_workers=1)\n",
    "\n",
    "print(\"trainset\", len(trainset), len(trainloader))\n",
    "print(\"validset\", len(validset), len(validloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(net, dloader, epoch):\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        valid_acc = 0\n",
    "        num_correct = 0\n",
    "        \n",
    "        for i, data in enumerate(dloader):\n",
    "            inputs, labels = data[:, :-1].cuda(), data[:, -1].long().cuda()\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # loss\n",
    "            valid_loss += criterion(outputs, labels).item() * data.shape[0]\n",
    "\n",
    "            # acc\n",
    "            (_, arg_maxs) = Tensor.max(outputs.data, dim=1)\n",
    "            num_correct += Tensor.sum(labels==arg_maxs)\n",
    "    \n",
    "    acc = (num_correct * 100.0 / len(validset)).item()\n",
    "    loss = valid_loss/len(validset)\n",
    "    print(\"[Epoch %d] Eval Acc %.2f%% (Loss %.3f)\" % (epoch, acc, loss))\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "    def __init__(self, n_inputs):\n",
    "        super(Net1, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_inputs, 34)\n",
    "        self.fc2 = nn.Linear(34, 50)\n",
    "        self.fc3 = nn.Linear(50, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class PENet2(nn.Module):\n",
    "    def __init__(self, n_inputs):\n",
    "        super(PENet2, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_inputs, 60)\n",
    "        self.fc2 = nn.Linear(60, 150)\n",
    "        self.fc3 = nn.Linear(150, 250)\n",
    "        self.fc4 = nn.Linear(250, 300)\n",
    "        self.fc5 = nn.Linear(300, 500)\n",
    "        self.fc6 = nn.Linear(500, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "    \n",
    "class PENet3(nn.Module):\n",
    "    def __init__(self, n_inputs):\n",
    "        super(PENet3, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_inputs, 60)\n",
    "        self.fc2 = nn.Linear(60, 150)\n",
    "        self.fc3 = nn.Linear(150, 250)\n",
    "        self.fc4 = nn.Linear(250, 300)\n",
    "        self.fc5 = nn.Linear(300, 500)\n",
    "        self.fc6 = nn.Linear(500, 700)\n",
    "        self.fc7 = nn.Linear(700, 500)\n",
    "        self.fc8 = nn.Linear(500, 300)\n",
    "        self.fc9 = nn.Linear(300, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x = F.relu(self.fc8(x))\n",
    "        x = self.fc9(x)\n",
    "        return x\n",
    "\n",
    "net = PENet2(n_inputs=pos_enc_size+40).cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.AdamW(net.parameters(), lr=1e-3)\n",
    "last_saved_eval_acc = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "print(\"Total params\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.AdamW(net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_iter = int(150000/batch_size)\n",
    "\n",
    "class_str = str(net.__class__).replace(\"'>\", \"\").split(\".\")[1]\n",
    "# timestamp = \"\"\n",
    "# CHECKPOINT_PATH = \"nn_checkpoints/40feat_unique_81e8_FConly_%.0f_%s_%s.pth\" % (last_saved_eval_acc, class_str, timestamp)\n",
    "# net.load_state_dict(torch.load(CHECKPOINT_PATH))\n",
    "\n",
    "for epoch in range(1, 500):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[:, :-1].cuda(), data[:, -1].long().cuda()\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % print_iter == print_iter-1:    # print every 20 mini-batches\n",
    "            print('[%d, %5d] loss: %.4f' %\n",
    "                  (epoch + 1, i + 1, running_loss / print_iter))\n",
    "            running_loss = 0.0\n",
    "    eval_acc, _ = eval_epoch(net, validloader, epoch)\n",
    "    if int(eval_acc) > last_saved_eval_acc:\n",
    "        last_saved_eval_acc = int(eval_acc)\n",
    "        CHECKPOINT_PATH = \"nn_checkpoints/40feat_unique_81e8_FConly_%.0f_%s_%s.pth\" % (last_saved_eval_acc, class_str, time.time())\n",
    "        torch.save(net.state_dict(), CHECKPOINT_PATH)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
